---
title: "Text Mining Doc"
author: "NiccolÃ² Salvini"
output: html_document
---

```{r global.options, include=FALSE}

knitr::opts_chunk$set(
  warning = FALSE,                    # Whether to preserve warnings
  message = FALSE,                    # Whether to preserve messages emitted
  echo = FALSE,                       # Whether to display the source code in the output document.
  cache = TRUE,                       # Whether to cache a code chunk, When evaluating code chunks for the second time, the cached chunks are skipped (unless they have been modified), but the objects created in these chunks are loaded from previously saved databases (.rdb and .rdx files), and these files are saved when a chunk is evaluated for the first time, or when cached files are not found (e.g., you may have removed them by hand). 
  strip.white = TRUE,                 # if FALSE knitr will not remove white spaces at the beg or end 
  fig.width=5,                        # the Width for plots created by code chunk
  fig.height=7,                       # the Height for plots created by code chunk
  cache = FALSE,                      # if TRUE knitr will cache the results to reuse in future knits
  collapse = TRUE,
  fig.path = "figures/",              # If we need to plot something from here images are redirected to cd - figures/
  dpi = 300,
  cache.lazy = FALSE,
  tidy = "styler",
  out.width = "90%",
  fig.align = "center"
)

## load libraries
if (!require("pacman")) install.packages("pacman")

pacman::p_load(tidyverse, tidytext, wordcloud, reshape2, scales, magick, ggplot2, latex2exp, pdftools, purrrlyr)

# ## base_color = "#1c5253",
# header_font_google = google_font("Rubik"),
# text_font_google   = google_font("Rubik", "300", "300i")
# code_font_google   = google_font("Fira Mono"),
# 

```


```{r loadddata}

library(forcats, warn.conflicts = F, quietly = T)

francia = pdf_text("data/francia.pdf") %>%
  readr::read_lines() %>%  
  str_squish() %>% 
  as_tibble() %>% 
  mutate(paese= forcats::as_factor("francia")) %>% 
  rename(text = value)

germania = pdf_text("data/germania.pdf") %>%
  readr::read_lines() %>%
   str_squish() %>%  
  as_tibble() %>% 
  mutate(paese= forcats::as_factor("germania")) %>% 
  rename(text = value)

both_pnr = bind_rows(francia,germania)
```



```{r theme1}

## theme version 1 for ggplot2 output plots
## 
theme_oi1 = function(){ 
    font = "Rubick"   #assign font family up front
    
    theme_light() %+replace%    #replace elements we want to change
    
    theme(
      
      #grid elements
      panel.grid.major = element_blank(),    #strip major gridlines
      panel.grid.minor = element_blank(),    #strip minor gridlines
      axis.ticks = element_blank(),          #strip axis ticks
      
      #since theme_minimal() already strips axis lines, 
      #we don't need to do that again
      
      #text elements
      plot.title = element_text(             #title
                   family = font,            #set font family
                   size = 20,                #set font size
                   face = 'bold',            #bold typeface
                   hjust = 0,                #left align
                   vjust = 2),               #raise slightly
      
      plot.subtitle = element_text(          #subtitle
                   family = font,            #font family
                   size = 14),               #font size
      
      plot.caption = element_text(           #caption
                   family = font,            #font family
                   size = 9,                 #font size
                   hjust = 1),               #right align
      
      axis.title = element_text(             #axis titles
                   family = font,            #font family
                   size = 10),               #font size
      
      axis.text = element_text(              #axis text
                   family = font,            #axis famuly
                   size = 9),                #font size
      
      axis.text.x = element_text(            #margin for axis text
                    margin=margin(5, b = 10))
      
      #since the legend often requires manual tweaking 
      #based on plot content, don't define it here
    )
}


```


```{r theme2}

## theme version 2 for ggplot2 output plots
## I honestly like it more this way 
##
theme_oi2 = function(family = "Rubick") {
  list(ggplot2::`%+replace%`(
    ggplot2::theme_grey(base_size = 11.5, base_family = family),
    ggplot2::theme(
      # add padding to the plot
      plot.margin = grid::unit(rep(0.5, 4), "cm"),

      # remove the plot background and border
      plot.background = ggplot2::element_blank(),
      panel.background = ggplot2::element_blank(),
      panel.border = ggplot2::element_blank(),

      # make the legend and strip background transparent
      legend.background = ggplot2::element_rect(fill = "transparent",
                                                colour = NA),
      legend.key = ggplot2::element_rect(fill = "transparent",colour = NA),
      strip.background = ggplot2::element_rect(fill = "transparent",
                                               colour = NA),

      # add light, dotted major grid lines only
      panel.grid.major = ggplot2::element_line(linetype = "dotted",
                                               colour = "#1c5253",
                                               size = 0.3),
      panel.grid.minor = ggplot2::element_blank(),

      # remove the axis tick marks and hide axis lines
      axis.ticks = ggplot2::element_blank(),
      axis.line = ggplot2::element_line(color = "#1c5253", size = 0.3),

      # modify the bottom margins of the title and subtitle
      plot.title = ggplot2::element_text(size = 18, colour = "#1c5253",
                                         hjust = 0.5,
                                         margin = ggplot2::margin(b = 10)),
      plot.subtitle = ggplot2::element_text(size = 12, colour = "#1c5253",
                                            hjust = 0.5,
                                            margin = ggplot2::margin(b = 10)),

      # add padding to the caption
      plot.caption = ggplot2::element_text(size = 10, colour = "#1c5253",
                                           hjust = 1,
                                           margin = ggplot2::margin(t = 15)),

      # Adjust text size and axis title position
      axis.title = ggplot2::element_text(size = 13, colour = "#1c5253",
                                         hjust = 0.95),
      axis.text = ggplot2::element_text(size = 10, colour = "#212121"),
      legend.title = ggplot2::element_text(size = 12, colour = "#1c5253"),
      legend.text = ggplot2::element_text(size = 10, colour = "#1c5253"),
      strip.text = ggplot2::element_text(size = 12, colour = "#1c5253", 
                                         margin = ggplot2::margin(10, 10, 
                                                                  10, 10, 
                                                                  "pt"))
    )
  ))
}


```


## A couple of refs 


```{r couple_rfs}

knitr::include_url(url = "https://www.business-science.io/business/2020/09/24/using-drake-etl.html?utm_content=buffer092bf&utm_medium=social&utm_source=linkedin.com&utm_campaign=buffer")

knitr::include_url(url = "https://cbail.github.io/textasdata/word2vec/rmarkdown/word2vec.html")





``` 

## A few first tidy text mining examples


```{r}

library(dplyr)
library(stringr)


pnrs = both_pnr %>%
  group_by(paese) %>%
  mutate(line = row_number()) %>% 
  ungroup()

pnrs

```




```{r}
library(tidytext)
tidy_pnrs <- pnrs %>%
  unnest_tokens(word, text)

tidy_pnrs
```



```{r clean}

clean_pnrs <- tidy_pnrs %>%
  anti_join(get_stopwords(language = "fr")) %>%
  anti_join(get_stopwords(language = "en"))

clean_pnrs %>% filter(paese =="francia")
```


```{r, include=FALSE}
positive <- get_sentiments("bing") %>%
  filter(sentiment == "positive")

clen_pnrs %>%
  filter(paese == "germania") %>%
  semi_join(positive) %>%
  count(word, sort = TRUE)
```



```{r}
library(tidyr)
library(rfeel)

bing = get_sentiments("bing")
fr_sent = rfeel("polarity")
  

## modo alternativo unico 
# pnr_sentiments <- clean_pnrs %>%
#   inner_join(bing) %>%
#   inner_join(fr_sent, ) %>%
#   count(paese, index = line %/% 80, sentiment) %>%
#   spread(sentiment, n, fill = 0) %>%
#   mutate(sentiment = positive - negative)
# 


fr_stp = francia %>% 
  unnest_tokens(word, text) %>% 
  anti_join(get_stopwords(language = "fr")) 
  

ger_stp = germania %>% 
  unnest_tokens(word, text) %>% 
  anti_join(get_stopwords(language = "en")) 

fr_stp_sent = fr_stp %>% 
  inner_join(fr_sent) %>% 
  rename(sentiment=polarity)
  
ger_stp_sent = ger_stp %>% 
  inner_join(bing) 

cleaned_pnrs = bind_rows(fr_stp_sent,ger_stp_sent )

cleaned_pnrs = cleaned_pnrs %>% 
  mutate(line = row_number()) %>% 
  count(paese, index = line %/% 30, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)



```



```{r}
library(magick, warn.conflicts = F, quietly = T)
# library(extrafont, warn.conflicts = F, quietly = T)
# extrafont::font_import()
# extrafont::loadfonts()

logo = image_read("img/officine_italia.png")
grid::grid.raster(logo, x = 0.09, y = 0, just = c('left', 'bottom'), width = unit(1.5, 'inches'))


ggplot(cleaned_pnrs, aes(index, sentiment, fill = paese)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~paese, ncol = 2, scales = "free_x") +
  theme_oi2()

ggsave("figures/sent_per_doc.pdf", dpi = "retina")
```


## Most common positive and negative words


```{r assocCount}
cleaned_pnrs = bind_rows(fr_stp_sent,ger_stp_sent )

bing_word_counts <- cleaned_pnrs %>%
  count(word, sentiment, paese,sort = TRUE)

bing_word_counts
```


```{r, fig.cap="Each contribution to Sentiment per word facetted for Negative and Positive"}
## for german
bing_word_counts %>%
  filter(paese =="germania") %>% 
  filter(n > 15) %>% ## qui tiri un po' giÃ¹ perchÃ¨ ci sono meno parole
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col() +
  coord_flip() +
  labs(y = "Contribution to sentiment")+
  theme_oi2()

## for france
bing_word_counts %>%
  filter(paese =="francia") %>% 
  filter(n > 150) %>%
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col() +
  coord_flip() +
  labs(y = "Contribution to sentiment")+
  theme_oi2()


```

## Wordclouds b&n

```{r wordcloudb&n}
library(wordcloud, warn.conflicts = F, quietly = T)
## B&W wordcloud for francia
cleaned_pnrs %>%
  filter(paese == "francia") %>% 
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))

## B&W wordcloud for germania
cleaned_pnrs %>%
  filter(paese == "germania") %>% 
  count(word) %>%
  with(wordcloud(word, n, max.words = 200))

```

## Wordclouds colour 

```{r wordcloudcolour}

## colored wordcloud for germania 
cleaned_pnrs %>% 
  filter(paese == "germania") %>% 
  inner_join(bing) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("#F8766D", "#00BFC4"),
                   max.words = 150)

## colored wordcloud for francia
cleaned_pnrs %>% 
  filter(paese == "francia") %>% 
  inner_join(fr_sent) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("#F8766D", "#00BFC4"),
                   max.words = 100)


## problemi col save perchÃ¨ non Ã¨ stack ggplot 
## e in piÃ¹ problemi di import dei fonts
## 

## Tento l'approccio funzionale 
## 

gen_wordcloud_sent = function(cleaned_docs, country, max.words) {
  
  stopifnot(is.numeric(max.words))
  if(identical(names(cleaned_pnrs),c("paese","word","sentiment"))){
    invisible()
  } else {
    message("Error: colums 'paese','word','sentiment' are either missing 
            in esamble or are not named correctly")
  }
  
  cleaned_docs %>%
    filter(paese == country) %>% 
    inner_join(bing) %>%
    count(word, sentiment, sort = TRUE) %>%
    acast(word ~ sentiment, value.var = "n", fill = 0) %>%
    comparison.cloud(colors = c("#F8766D", "#00BFC4"),
                     max.words =  as.numeric(max.words))

    
}

# gen_wordcloud_sent(cleaned_docs = cleaned_pnrs, country = "germania", max.words = 100)


```


## Getting Trigrams

```{r gerTrigramstart}
# param set
n_word <- 20
n_top <- 150
n_gramming <- 3
 
## Trigrams fro German PNR 
trigrams <- germania %>%
  unnest_tokens(trigram, text, token = "ngrams", n = n_gramming)

start_words <- c("young", "pension")

## verifica presenza parole   
## 
# germania %>% 
#   filter(str_detect(text, "pension"))

```



```{r gerTrigrammean}
## top 150 trigrams using count and some regex magi
pattern <- str_c("^", start_words, " ", collapse = "|")
top_words <- trigrams %>%
  filter(str_detect(trigram, pattern)) %>%
  count(trigram, sort = TRUE) %>%
  slice(seq_len(n_top)) %>%
  pull(trigram)
trigrams <- trigrams %>%
  filter(trigram %in% top_words)
## trova i nodi (stopworded)
str_nth_word <- function(x, n, sep = " ") {
  str_split(x, pattern = " ") %>%
  map_chr(~ .x[n])
}
nodes <- map_df(seq_len(n_gramming),
       ~ trigrams %>%
           mutate(word = str_nth_word(trigram, .x)) %>%
           count(word, sort = TRUE) %>%
           slice(seq_len(n_word)) %>% 
           mutate(y = seq(from = n_word + 1, to = 0, length.out = n() + 2)[seq_len(n()) + 1],
                  x = .x)) %>%
  anti_join(get_stopwords(language = "en"))
## plot of node positions (check point)
# nodes %>% 
#   ggplot(aes(x, y, label = word))+
#   geom_text()
  
sigmoid <- function(x_from, x_to, y_from, y_to, scale = 5, n = 100) {
  x <- seq(-scale, scale, length = n)
  y <- exp(x) / (exp(x) + 1)
  tibble(x = (x + scale) / (scale * 2) * (x_to - x_from) + x_from,
         y = y * (y_to - y_from) + y_from)
}

egde_lines <- function(trigram, from_word, to_word, scale = 5, n = 50, 
                       x_space = 0) {
  from_word <- from_word %>%
    select(-n) %>%
    set_names(c("from", "y_from", "x_from"))
  
  to_word <- to_word %>%
    select(-n) %>%
    set_names(c("to", "y_to", "x_to"))
  
  links <- crossing(from = from_word$from, 
                    to = to_word$to) %>%
    mutate(word_pair = paste(from, to),
           number = map_dbl(word_pair, 
                            ~ sum(str_detect(trigram$trigram, .x)))) %>%
    left_join(from_word, by = "from") %>%
    left_join(to_word, by = "to")
  
  links %>%
    by_row(~ sigmoid(x_from = .x$x_from + 0.2 + x_space,
                     x_to = .x$x_to - 0.05, 
                     y_from = .x$y_from, y_to = .x$y_to, 
                     scale = scale, n = n) %>%
    mutate(word_pair = .x$word_pair,
           number = .x$number,
           from = .x$from)) %>%
    pull(.out) %>%
    bind_rows()
}
### check up edges 
### 
# egde_lines(trigram = trigrams, 
#            from_word = filter(nodes, x == 1),   
#            to_word = filter(nodes, x == 2)) %>%
#   filter(number > 0) %>%
#   ggplot(aes(x, y, group = word_pair, alpha = number, color = from)) +
#   geom_line()
#   
```

```{r calcedges}
# egdes between first and second column
egde1 <- egde_lines(trigram = trigrams, 
           from_word = filter(nodes, x == 1), 
           to_word = filter(nodes, x == 2), 
           n = 50) %>%
           filter(number > 0) %>%
  mutate(id = word_pair)
# Words in second colunm
## That start with he
second_word_he <- nodes %>%
  filter(x == 2) %>%
  select(-n) %>%
  left_join(
    trigrams %>% 
      filter(str_nth_word(trigram, 1) == start_words[1]) %>%
      mutate(word = str_nth_word(trigram, 2)) %>%
      count(word), 
    by = "word"
  ) %>%
  replace_na(list(n = 0))
## That start with she
second_word_she <- nodes %>%
  filter(x == 2) %>%
  select(-n) %>%
  left_join(
    trigrams %>% 
      filter(str_nth_word(trigram, 1) == start_words[2]) %>%
      mutate(word = str_nth_word(trigram, 2)) %>%
      count(word), 
    by = "word"
  ) %>%
  replace_na(list(n = 0))
# Words in third colunm
## That start with he (stopworded)
third_word_he <- nodes %>%
  filter(x == 3) %>%
  select(-n) %>%
  left_join(
    trigrams %>% 
      filter(str_nth_word(trigram, 1) == start_words[1]) %>%
      mutate(word = str_nth_word(trigram, 3)) %>%
      count(word), 
    by = "word"
  ) %>%
  replace_na(list(n = 0)) %>%   
  anti_join(get_stopwords(language = "en"))
## That start with she (stopworded)
third_word_she <- nodes %>%
  filter(x == 3) %>%
  select(-n) %>%
  left_join(
    trigrams %>% 
      filter(str_nth_word(trigram, 1) == start_words[2]) %>%
      mutate(word = str_nth_word(trigram, 3)) %>%
      count(word), 
    by = "word"
  ) %>%
  replace_na(list(n = 0)) %>%
  anti_join(get_stopwords(language = "en"))
# egdes between second and third column that starts with he
egde2_he <- egde_lines(filter(trigrams, 
                              str_detect(trigram, paste0("^", start_words[1], " "))), 
             second_word_he, third_word_he, n = 50) %>%
  mutate(y = y + 0.05,
         from = start_words[1],
         id = str_c(from, word_pair, sep = " ")) %>%
  filter(number > 0)
# egdes between second and third column that starts with she
egde2_she <- egde_lines(filter(trigrams, 
                              str_detect(trigram, paste0("^", start_words[2], " "))), 
             second_word_she, third_word_she, n = 50) %>%
  mutate(y = y - 0.05,
         from = start_words[2],
         id = str_c(from, word_pair, sep = " ")) %>%
  filter(number > 0)
# All edges
edges <- bind_rows(egde1, egde2_he, egde2_she)

p <- nodes %>% 
  ggplot(aes(x, y, label = word, size = n)) +
  geom_text(hjust = 0, color = "white") +
  theme_void() +
  geom_line(data = edges,
            aes(x, y, group = id, color = from, alpha = sqrt(number)),
            inherit.aes = FALSE) +
  guides(alpha = "none", color = "none", size = "none") +
  xlim(c(0.9, 3.2)) +
  scale_size(range = c(3, 8)) +
  scale_color_manual(values = c("#5EF1F1", "red")) +
  theme(plot.background = element_rect(fill ="#1c5253", colour = 'black'),
        text = element_text(color = "#1c5253", size = 15))
p

ggsave("figures/trigrams_ger.pdf", dpi = "retina")
```




## Topic Mining 

goal: quali sono i maggiori temi per sezione (divisione del docuemento)

```{r topicmodels}
library(topicmodels, warn.conflicts = F, quietly = T)

## Secondo metodo non supervisionato 
## 
# 
# sec_by_word_ger = germania %>%
#   unnest_tokens(word, text) %>%
#   anti_join(get_stopwords(language = "en")) %>%  
#   mutate(sect = cumsum(str_detect(
#     text, regex("^chapter ", ignore_case = TRUE)
#   )))
# 
# 
# ger_by_sec = germania %>%
#   mutate(
#     sect = case_when(
#       str_detect(text, regex("^chapter ", ignore_case = TRUE)) ~ "summary",
#       str_detect(text, regex("^chapter ", ignore_case = TRUE)) ~ "summary",
#       str_detect(text, regex("^chapter ", ignore_case = TRUE)) ~ "summary",
#       str_detect(text, regex("^chapter ", ignore_case = TRUE)) ~ "summary",
#       str_detect(text, regex("^chapter ", ignore_case = TRUE)) ~ "summary",
#       str_detect(text, regex("^chapter ", ignore_case = TRUE)) ~ "summary"
#     )
#   ) 


## labelling manuale 

ger_sec_1 = pdftools::pdf_text("data/germania.pdf")[7:9] %>%  
  readr::read_lines() %>% 
  str_squish() %>% 
  as_tibble() %>% 
  mutate(paese= forcats::as_factor("germania"),
         sect = forcats::as_factor("Summary")) %>% 
  rename(text = value)

ger_sec_2 = pdftools::pdf_text("data/germania.pdf")[11] %>%  
  readr::read_lines() %>% 
  str_squish() %>% 
  as_tibble() %>% 
  mutate(paese= forcats::as_factor("germania"),
         sect = forcats::as_factor("Financial framework")) %>% 
  rename(text = value)

ger_sec_3 = pdftools::pdf_text("data/germania.pdf")[13:15] %>%  
  readr::read_lines() %>% 
  str_squish() %>% 
  as_tibble() %>% 
  mutate(paese= forcats::as_factor("germania"),
         sect = forcats::as_factor("Linking the plan with the European Semester")) %>% 
  rename(text = value)

ger_sec_4 = pdftools::pdf_text("data/germania.pdf")[17:19] %>%  
  readr::read_lines() %>% 
  str_squish() %>% 
  as_tibble() %>% 
  mutate(paese= forcats::as_factor("germania"),
         sect = forcats::as_factor("Description of investment measures and reforms")) %>% 
  rename(text = value)

ger_sec_5 = pdftools::pdf_text("data/germania.pdf")[21:41] %>%  
  readr::read_lines() %>% 
  str_squish() %>% 
  as_tibble() %>% 
  mutate(paese= forcats::as_factor("germania"),
         sect = forcats::as_factor("Details of focus areas for measures and components")) %>% 
  rename(text = value)

ger_sec_6 = pdftools::pdf_text("data/germania.pdf")[43] %>%  
  readr::read_lines() %>% 
  str_squish() %>% 
  as_tibble() %>% 
  mutate(paese= forcats::as_factor("germania"),
         sect = forcats::as_factor("Institutional governance of the German Recovery and Resilience Plan")) %>% 
  rename(text = value)


ger_doc = bind_rows(ger_sec_1, ger_sec_2, ger_sec_3, ger_sec_4, ger_sec_5, ger_sec_6)


ger_by_sec_word = ger_doc %>% 
  unnest_tokens(word, text) %>% 
  anti_join(get_stopwords(language = "en")) %>% 
  count(sect, word, sort = TRUE)

dtm_ger = ger_by_sec_word %>% 
  cast_dtm(sect, word, n)

library(glue, warn.conflicts = F, quietly = T)

## fitting model
ger_lda <- LDA(dtm_ger, k = 7, control = list(seed = 2021))
ger_lda_tidy <- tidy(ger_lda, matrix = "beta")
ger_lda_tidy %>%  head(15)
```

Nota che a questo punto l'operazione ha cambaito il modello nel foprmato: 1-argomento-per-riga. PEr ogni combinazione il modello calcola la probabilita che quel termina veng da quel preciso argomento. Per esempio il termine "digital" ha probabilitÃ  `r glue("{ger_lda_tidy %>%  select(beta) %>%  slice(1) %>%  round(3)}%")` di appartenere all'arogmento 1, e invece probavilitÃ  `r glue("{ger_lda_tidy %>%  select(beta) %>%  slice(2) %>%  round(3)}%")` di appartenere al 2, probabilitÃ  `r glue("{ger_lda_tidy %>%  select(beta) %>%  slice(3) %>%  round(3)}%")` di appartenere al 3.

Vedo le top 10 parole e posso desumere che il tema a SX riguarda **l'Economia-Finanza**, a DX **Politica**

```{r toptermds}

top_terms <- ger_lda_tidy %>%
  group_by(topic) %>%
  slice_max(beta, n = 10, with_ties = FALSE) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()+ 
  xlab(" ") +
  ylab(" ") +
  theme(axis.text.x = element_text(angle = 25, vjust = 0.5, hjust=1),
        plot.background = ggplot2::element_blank(),
        panel.background = ggplot2::element_blank())


## per un tema diverso 
# theme_oi2( axis.text.x = element_text(angle = 25, vjust = 0.5, hjust=1)) +
# ylab(" ")
# 

```


##  Ma puoi anche fare cosi'

Puoi anche andare a vedere le differenze maggiori tra score di probabilitÃ , chiamala $\beta$, tra le parole e andarle a comparare nuovamente i due topics. In questa maniera filtri sole le parole meno comuni, quindi quelle potenzialmente piÃ¹ pregne di significato.

```{r betastopic, include=F}

## under development 
## 

beta_spread <- top_terms %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > .001 | topic2 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1))

beta_spread %>%
  group_by(direction = log_ratio > 0) %>%
  top_n(10, abs(log_ratio)) %>%
  ungroup() %>%
  mutate(term = reorder(term, log_ratio)) %>%
  ggplot(aes(log_ratio, term)) +
  geom_col() +
  labs(
    x = "Log2 odds ratio dei betas tra topic 2 / topic 1",
    y = ""
    )+ 
  theme_oi2()

```


## network di bigrammi 

```{r bigramsnet}


library(ggraph, warn.conflicts = F, quietly = T)
library(igraph, warn.conflicts = F, quietly = T)


austen_bigrams <- austen_books() %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

bigrams_separated <- austen_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

bigrams_united %>% 
  head(30) %>% 
    reactable(searchable = TRUE,
            minRows = 8,
            theme = reactableTheme(
    borderColor = "#1c5253",
    stripedColor = "#1c5253",
    highlightColor = "#1c5253",
    cellPadding = "8px 12px",
    style = list(fontFamily = "-apple-system, BlinkMacSystemFont, Segoe UI, Helvetica, Arial, sans-serif"),
    searchInputStyle = list(width = "100%")
  ))

#########################
#########################

ger_bigram = germania %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)


bigrams_separated_ger <- ger_bigram %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated_ger %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)
set.seed(2021)

bigram_graph <- bigram_counts %>%
  filter(n > 5) %>%
  graph_from_data_frame()


a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()


## approccio funzionale al network di bigrammi 
## 




```




